My attempt at implementing the Proximal Policy Optimization algorithm from scratch for Gymnasium's Atari environments, using PyTorch.

However, despite numerous debugging attempts, some hyperparameter tuning and tweaks in the implementation, I was unable to get it to converge.

I ultimately wanted to compare it with SB3's PPO, but my model diverges.

Help in fixing my code would be greatly appreciated. Thank you.
